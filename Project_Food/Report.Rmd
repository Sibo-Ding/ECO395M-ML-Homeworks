---
title: "DM&SL Project Food"
author: "Sibo Ding"
output: md_document
---

# Estimate and Predict my Food Pattern in Austin Using Data Wrangling and Machine Learning

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(dplyr)
library(tidyr)
library(caret)
library(nnet)  # Multinomial logistic regression
library(e1071)  # Naive Bayes
library(randomForest)
library(xgboost)
```

## Abstract









## Introduction
I started to record my everyday meals in July 2022 by a very incidental chance. Since then, I spent most of my time in Hong Kong until I moved to Austin in July 2023. From my experience in Hong Kong, I was wondering whether people tend to eat better (to relax or to compensate) or simpler (to save time) when they are busy. However, my life and food patterns in Hong Kong were too complicated and unpredictable to verify this hypothesis. Considering the feasibility, I decide to estimate and predict my life and food patterns in Austin.  

When recording meals, there are potential discrepancies and biases due to my discretion. For example, if I have a brunch (at 10:00) and an afternoon tea (at 16:00), sometimes I may record them as breakfast and lunch, but I may also record them as lunch and dinner. Another discrepancy is the vague distinction between snacks and meals. If I consider 10 g popcorn as a snack, should I consider 11 g as a meal? If so, then what about 10.1 g, 10.11 g, or 10 g rice, etc.?  

Beyond the discrepancies, I am not very confident in the predictive accuracy for two additional reasons. First, the data set is small. Second, although my life in Austin is simple due to some constraints, the data is from a real human with certain flexibility and unpredictability. However, it is still fun to know the driving factors of my life and food patterns.  

## Methods
### Data wrangling
I keep `date` when I am in Austin: after Jul 4, 2023 (inclusive), exclude the Thanksgiving holiday (from Nov 20 to Nov 26, both inclusive) and winter vacation (from Dec 12, 2023 to Jan 11, 2024, both inclusive). The initial data looks like this:
```{r Read and display data}
df <- read.csv("Food.csv", na.strings = "")

df |>
  select(date, dow, breakfast, lunch, dinner) |>
  head() |>
  knitr::kable()
```

During this time, I am studying at The University of Texas at Austin, so my life pattern heavily depends on the school calendar. Thus, I create a categotical variable `semester`: it is *summer* when `date` is before Aug 14 (inclusive), *fall* when `date` is after Aug 15 and before Dec 11 (both inclusive), and *spring* otherwise.  

For the same reason, I create a categorical variable `week_of_sem`, where the first week of a semester is 1, the second is 2, etc. Every week starts on Monday or the first day of a semester if that day is not a Monday. I set non-school days as 0, including spring break and days before or after each semester.  

The variation in `breakfast` is close to zero as I eat at home most of the time. To extract useful information, I convert `breakfast` to a binary variable `breakfast_or_not`, because having breakfast may indicate going out, and its food pattern may be different from staying at home.  
```{r Data wrangling 1}
# Create a categorical column
df <- df |>
  mutate(breakfast_or_not = case_when(
    is.na(breakfast) ~ 0,
    TRUE ~ 1) |> factor())
```

I convert the data frame from wide format to long format.  
```{r Data wrangling 2}
# Convert wide data to long data
# Drop N/A in `food`
df <- df |>
  pivot_longer(cols = c(lunch, dinner),
               names_to = "meal", 
               values_to = "food") |>
  filter(!is.na(food))
```

I am interested in estimating and predicting my food pattern. For ease of implementation, I create a categorical variable `food_class` representing 3 `food` categories: *home*, *canteen* (including *J2 Dining*, *Jester City Limits*, and *Kins Dining*), and *other*.  
```{r Data wrangling 3}
# Create a categorical column
df <- df |>
  mutate(food_class = case_when(
    food == "Home" ~ "home",
    food %in% c("J2 Dining", "Jester City Limits", "Kins Dining") ~ "canteen",
    TRUE ~ "other") |> factor())
```

Previous meals have impacts on the choice of the next meal. On one hand, I may get bored with previous meals (diminishing marginal return). On the other hand, I may be reluctant or constrained to change life and food patterns. Therefore, I create four lagging variables of `food_class`. I drop the first four observations and the first four observations after winter break.  
```{r Data wrangling 4}
# Create 4 lags of `food_class`
df <- df |>
  mutate(food_class_l1 = lag(food_class)) |>
  mutate(food_class_l2 = lag(food_class, n = 2)) |>
  mutate(food_class_l3 = lag(food_class, n = 3)) |>
  mutate(food_class_l4 = lag(food_class, n = 4)) |>
  # Drop first 4 observations
  filter(!is.na(food_class_l4)) |>
  # Drop first 4 observations after winter break
  filter(!date %in% c("1/12/2024", "1/13/2024", "1/14/2024"))
```

Here is the data after all processing:
```{r Data wrangling 5}
df1 <- df |>
  # Select columns
  select(food_class,
         food_class_l1, food_class_l2, food_class_l3, food_class_l4,
         meal, semester, week_of_sem, dow, breakfast_or_not) |>
  # Convert data types of columns
  mutate(across(
    c(meal, semester, week_of_sem, dow), as.factor))

head(df1) |> knitr::kable()
```

### Predict my food pattern using classification models
The outcome variable (y variable) `food_class` is categorical. Before any analysis, here is the number of observations in each category:
```{r}
df1 |>
  group_by(food_class) |>
  count() |>
  rename(count = n) |>
  knitr::kable()
```

I do not scale or normalize the data, as all features (x variables) are categorical.  

I set 80% of the observations as training data, and 20% as test data.
```{r Train-test split}
set.seed(123)
index <- createDataPartition(df1$food_class ,
                             p = 0.8, list = FALSE)
train <- df1[index, ]
test <- df1[-index, ]
```

I fit classification models with training data, predict outcomes on test data, and compare the predicted outcomes to the actual outcomes.

#### Logistic regression
I include all features and most of their interactions in logistic regression. The reason for including interactions is, for example, a lunch on Monday may differ from one on Saturday, depending on my class schedule. For easy computation, I omit the interaction between `week_of_sem` and `dow`, and the interaction between `semester` and `week_of_sem`. I use softmax function to handle 3 classes in the outcome variable.
```{r Logistic regression 1, results='hide'}
# Fit multinomial logistic regression
logit <- multinom(food_class ~
                    . ^2 - week_of_sem*dow - semester*week_of_sem,
                  data = train)
# Predict classes
logit_pred <- predict(logit, test)
# Confusion matrix
logit_cm <- confusionMatrix(logit_pred, test$food_class)
```

#### Lasso regularization
Logistic regression with too many features may result in overfitting. Thus, I use lasso to regularize the above model. I use 10-fold cross validation in the training data to find the optimal regularization parameter $\lambda$.
```{r Lasso regularization 1, results='hide'}
# 10-fold cross validation
ctrl <- trainControl(method = "cv", number = 10)

set.seed(12)
lasso <- train(food_class ~
                 . ^2 - week_of_sem*dow - semester*week_of_sem,
               data = train,
               method = "multinom",
               trControl = ctrl,
               alpha = 1)
lasso_pred <- predict(lasso, test)
lasso_cm <- confusionMatrix(lasso_pred, test$food_class)
```

#### Naive Bayes
Naive Bayes assumes every feature is independent of all other features, conditional on the class labels of the outcome variable. This assumption contradicts the assumption of interactions in the logistic regression section above. However, it is still worth a try to fit a Naive Bayes model with all features.
```{r Naive Bayes 1}
nb <- naiveBayes(food_class ~ ., data = train)
nb_pred <- predict(nb, test)
nb_cm <- confusionMatrix(nb_pred, test$food_class)
```

#### KNN
KNN measures "distances" between features, which is not strictly appropriate for this data set with categorical features since the distances between categories are not clear. However, it is still worth a try to fit a KNN model with all features. I use 10-fold cross validation in the training data to find the optimal number of neighbors $k$.
```{r KNN 1}
# 10-fold cross validation is in Lasso chunk
set.seed(12)
knn <- train(food_class ~ .,
             data = train,
             method = "knn",
             trControl = ctrl)
knn_pred <- predict(knn, test)
knn_cm <- confusionMatrix(knn_pred, test$food_class)
```

#### Random forest
I include all features in random forest.
```{r Random forest 1}
rf <- randomForest(food_class ~ ., data = train)
rf_pred <- predict(rf, test)
rf_cm <- confusionMatrix(rf_pred, test$food_class)
```

#### CatBoost
CatBoost is a gradient boosting model for handling categorical features. I include all features in CatBoost.

```{r CatBoost 1, eval=FALSE}
# Output training and test data and fit CatBoost in Python
write.csv(train, "train.csv")
write.csv(test, "test.csv")
```

### Driving factors of my food pattern
Find dominant features among all features and their interactions.
```{r}
library(rpart)
library(rpart.plot)
tree <- rpart(food_class ~ . - week_of_sem, data = df1, maxdepth = 5)
rpart.plot(tree, type = 3, extra = 1, cex = 0.5)
```





### Comments on unsupervised learning



## Results
### Overall accuracies of classification models
```{r Overall accuracy}
data.frame(
  model = c(
    "Logistic regression",
    "Lasso regularization",
    "Naive Bayes",
    "KNN",
    "Random forest",
    "CatBoost"),
  overall_accuracy = c(
    logit_cm$overall[1],
    lasso_cm$overall[1],
    nb_cm$overall[1],
    knn_cm$overall[1],
    rf_cm$overall[1],
    0.7568) |> round(4)
) |>
  knitr::kable()



```

Overall accuracy measures the fraction of accurate predictions among all observations.  

CatBoost







## Conclusion



## Appendix
Below are confusion matrices of predictive models. In a confusion matrix, each column is an original class, each row is a predicted class.  

Logistic regression:
```{r Logistic regression 2}
logit_cm$table |> knitr::kable()
```

Lasso regularization:
```{r Lasso regularization 2}
lasso_cm$table |> knitr::kable()
```

Naive Bayes:
```{r Naive Bayes 2}
nb_cm$table |> knitr::kable()
```

KNN:
```{r KNN 2}
knn_cm$table |> knitr::kable()
```

Random forest:
```{r Random forest 2}
rf_cm$table |> knitr::kable()
```

CatBoost:
```{r CatBoost 2}
matrix(c("", "canteen", "home", "other",
         "canteen", 0, 0, 0,
         "home", 0, 0, 0,
         "other", 0, 0, 0),
       nrow = 4,
       ncol = 4) |>
  knitr::kable()
```








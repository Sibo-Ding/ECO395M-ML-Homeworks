---
title: "DM&SL Project Food"
author: "Sibo Ding"
output: md_document
---

# Estimate and Predict my Food Pattern in Austin Using Data Mining and Machine Learning

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(dplyr)
library(lubridate)
library(tidyr)
library(caret)
library(nnet)  # Multinomial logistic regression
library(e1071)  # Naive Bayes
library(randomForest)
```

## Abstract









## Introduction
I started to record my everyday meals in July 2022 by a very incidental chance. Since then, I spent most of my time in Hong Kong until I moved to Austin in July 2023. From my experience in Hong Kong, I was wondering whether people tend to eat better (to relax or to compensate) or simpler (to save time) when they are busy. However, my life and food patterns in Hong Kong were too complicated and unpredictable to verify this hypothesis. Considering the feasibility, I decide to estimate and predict my life and food patterns in Austin.  

I admit this estimation and prediction is not very extendable, as my life pattern is likely to change in one or two years after my study at The University of Texas at Austin. But it is still fun to know the driving factors of my life and food patterns during this time. I select this topic as it allows me to build a real-life data science project from scratch, literally mining data and building models.  

When recording meals, there are potential discrepancies and biases due to my discretion. For example, if I have a brunch (at 10:00) and an afternoon tea (at 16:00), sometimes I may record them as breakfast and lunch, but I may also record them as lunch and dinner. Another discrepancy is the vague distinction between snacks and meals. If I consider 10 g popcorn as a snack, should I consider 11 g as a meal? If so, then what about 10.1 g, 10.11 g, or 10 g rice, etc.? Beyond the discrepancies, I am not very confident in the predictive accuracy for two additional reasons. First, the data set is small. Second, although my life in Austin is simple due to some constraints, the data is from a real human with certain flexibility and unpredictability.

## Methods
### Data mining
I keep `date` when I am in Austin: after Jul 4, 2023 (inclusive), exclude the Thanksgiving holiday (from Nov 20 to Nov 26, both inclusive) and winter vacation (from Dec 12, 2023 to Jan 11, 2024, both inclusive). The initial data looks like this:
```{r Read and display data}
df <- read.csv("Food.csv", na.strings = "")

df |>
  select(date, dow, breakfast, lunch, dinner) |>
  head() |>
  knitr::kable()
```

During this time, I am studying at UT Austin, so my life pattern heavily depends on the school calendar. Thus, I create a categotical variable `semester`: it is *summer* when `date` is before Aug 14 (inclusive), *fall* when `date` is after Aug 15 and before Dec 11 (both inclusive), and *spring* otherwise.  

For the same reason, I create a categorical variable `week_of_sem`, where the first week of a semester is 1, the second is 2, etc. Every week starts on Monday or the first day of a semester if that day is not a Monday. I set non-school days as 0, including spring break and days before or after each semester.  

The variation in `breakfast` is close to zero as I eat at home most of the time. To extract useful information, I convert `breakfast` to a binary variable `breakfast_or_not`, because having breakfast may indicate going out, and its food pattern may be different from staying at home.  
```{r Cleaning 1: create breakfast}
# Create a categorical column
df <- df |>
  mutate(breakfast_or_not = case_when(
    is.na(breakfast) ~ 0,
    TRUE ~ 1) |> factor())
```

Sport is an important part of my life. Visiting sports facilities may signal certain life patterns, though patterns may differ between on-campus gyms and off-campus fields. I obtain my visiting records of sports facilities from [UT Recreational Sports](https://secure.rs.utexas.edu/store/index.php). I create a binary variable `gym_or_not`, indicating whether I visit sports facilities on a day.  
```{r Cleaning 2: create gym}
gym <- read.csv("GymVisit.csv")
gym1 <- data.frame(date = as.Date(mdy_hms(gym$time)),
                   gym_or_not = 1)

# Merge `df` with `gym`
df <- df |> mutate(date = mdy(date))
df <- df |>
  left_join(gym1, by = "date") |>
  replace_na(list(gym_or_not = 0)) |>
  mutate(gym_or_not = factor(gym_or_not))
```

I convert the data frame from wide format to long format.  
```{r Cleaning 3: wide to long}
# Convert wide data to long data
# Drop N/A in `food`
df <- df |>
  pivot_longer(cols = c(lunch, dinner),
               names_to = "meal", 
               values_to = "food") |>
  filter(!is.na(food))
```

I am interested in estimating and predicting my food pattern. For ease of implementation, I create a categorical variable `food_class` representing three `food` categories: *home*, *canteen* (including *J2 Dining*, *Jester City Limits*, and *Kins Dining*), and *other*.  
```{r Cleaning 4: create food class}
# Create a categorical column
df <- df |>
  mutate(food_class = case_when(
    food == "Home" ~ "home",
    food %in% c("J2 Dining", "Jester City Limits", "Kins Dining") ~ "canteen",
    TRUE ~ "other") |> factor())
```

Previous meals have impacts on the choice of the next meal. On one hand, I may get bored with previous meals (diminishing marginal return). On the other hand, I may be reluctant or constrained to change life and food patterns. Therefore, I create four lagging variables of `food_class`. I drop the first four observations and the first four observations after winter break.  
```{r Cleaning 5: create lags of food class}
# Create 4 lags of `food_class`
df <- df |>
  mutate(food_class_l1 = lag(food_class)) |>
  mutate(food_class_l2 = lag(food_class, n = 2)) |>
  mutate(food_class_l3 = lag(food_class, n = 3)) |>
  mutate(food_class_l4 = lag(food_class, n = 4)) |>
  # Drop first 4 observations
  filter(!is.na(food_class_l4)) |>
  # Drop first 4 observations after winter break
  filter(!date %in% c("1/12/2024", "1/13/2024", "1/14/2024"))
```

Here is the data after all processing:
```{r Cleaning 6}
df1 <- df |>
  # Select columns
  select(food_class,
         food_class_l1, food_class_l2, food_class_l3, food_class_l4,
         meal, semester, week_of_sem, dow, breakfast_or_not,
         gym_or_not) |>
  # Convert data types of columns
  mutate(across(
    c(meal, semester, week_of_sem, dow), as.factor))

head(df1) |> knitr::kable()
```

### Predict my food pattern using classification models
The outcome variable (y variable) `food_class` is categorical. Before any analysis, here is the number of observations in each category:
```{r Summary of outcome variable}
df1 |>
  group_by(food_class) |>
  count() |>
  rename(count = n) |>
  knitr::kable()
```

I do not scale or normalize the data, as all features (x variables) are categorical.  

I set 80% of the observations as training data, and 20% as test data. I fit classification models with training data, predict outcomes on test data, and compare the predicted outcomes to the actual outcomes.  
```{r Train-test split}
set.seed(123)
index <- createDataPartition(df1$food_class,
                             p = 0.8, list = FALSE)
train <- df1[index, ]
test <- df1[-index, ]
```

I try 6 models: logistic regression, lasso, Naive Bayes, KNN, random forest, and CatBoost.  

I include all features and most of their interactions in logistic regression. The reason for including interactions is, for example, a lunch on Monday may differ from one on Saturday, depending on my class schedule. For easy computation, I omit four interactions: `food_class_l4 * week_of_sem`, `semester * dow`, `week_of_sem * dow`, and `semester * week_of_sem`. I use softmax function to handle three classes in the outcome variable.  
```{r Logistic regression 1, results='hide'}
# Fit multinomial logistic regression
logit <- multinom(food_class ~
                    .^2 - food_class_l4*week_of_sem - semester*dow -
                    week_of_sem*dow - semester*week_of_sem,
                  data = train)
# Predict classes
logit_pred <- predict(logit, test)
# Confusion matrix
logit_cm <- confusionMatrix(logit_pred, test$food_class)
```

Logistic regression with too many features may result in overfitting. Thus, I use lasso to regularize the above model. I use 10-fold cross validation in the training data to find the optimal regularization parameter $\lambda$.  
```{r Lasso regularization 1, results='hide'}
# 10-fold cross validation
ctrl <- trainControl(method = "cv", number = 10)

set.seed(12)
lasso <- train(food_class ~
                 .^2 - food_class_l4*week_of_sem - semester*dow -
                 week_of_sem*dow - semester*week_of_sem,
               data = train,
               method = "multinom",
               trControl = ctrl,
               alpha = 1)
lasso_pred <- predict(lasso, test)
lasso_cm <- confusionMatrix(lasso_pred, test$food_class)
```

Naive Bayes assumes every feature is independent of all other features, conditional on the class labels of the outcome variable. This assumption contradicts the assumption of interactions in the logistic regression section above. However, it is still worth a try to fit Naive Bayes with all features.  
```{r Naive Bayes 1}
nb <- naiveBayes(food_class ~ ., data = train)
nb_pred <- predict(nb, test)
nb_cm <- confusionMatrix(nb_pred, test$food_class)
```

KNN measures "distances" between features, which is not strictly appropriate for this data set with categorical features since the distances between categories are not clear. However, it is still worth a try to fit KNN with all features. I use 10-fold cross validation in the training data to find the optimal number of neighbors $k$.  
```{r KNN 1}
# 10-fold cross validation is in Lasso chunk
set.seed(12)
knn <- train(food_class ~ .,
             data = train,
             method = "knn",
             trControl = ctrl)
knn_pred <- predict(knn, test)
knn_cm <- confusionMatrix(knn_pred, test$food_class)
```

I include all features in random forest.  
```{r Random forest 1}
set.seed(12)
rf <- randomForest(food_class ~ ., data = train)
rf_pred <- predict(rf, test)
rf_cm <- confusionMatrix(rf_pred, test$food_class)
```

CatBoost is a gradient boosting model for handling categorical features. I include all features in CatBoost.  
```{r CatBoost 1, eval=FALSE}
# Output training and test data and fit CatBoost in Python
write.csv(train, "train.csv", row.names = FALSE)
write.csv(test, "test.csv", row.names = FALSE)
```

### Driving factors of my food pattern
Find dominant features among all features and their interactions.
```{r Tree}
library(rpart)
library(rpart.plot)
tree <- rpart(food_class ~ . - week_of_sem, data = df1, maxdepth = 5)
rpart.plot(tree, type = 3, extra = 1, cex = 0.5)
```

Best subset selection

variable importance plot showing when we leave out a variable, how much does that increase mean squared error
```{r}
varImpPlot(rf)
```







## Results
### Overall accuracies of classification models
```{r Overall accuracy}
data.frame(
  model = c(
    "Logistic regression",
    "Lasso regularization",
    "Naive Bayes",
    "KNN",
    "Random forest",
    "CatBoost"),
  overall_accuracy = c(
    logit_cm$overall[1],
    lasso_cm$overall[1],
    nb_cm$overall[1],
    knn_cm$overall[1],
    rf_cm$overall[1],
    0.8077) |>
    round(4)
) |>
  knitr::kable()
```

Overall accuracy measures the fraction of accurate predictions among outcomes in test data.  

CatBoost







## Conclusion



## Appendix
Below are confusion matrices of predictive models. In a confusion matrix, each column is an original class, each row is a predicted class.  

Logistic regression:
```{r Logistic regression 2}
logit_cm$table |> knitr::kable()
```

Lasso regularization:
```{r Lasso regularization 2}
lasso_cm$table |> knitr::kable()
```

Naive Bayes:
```{r Naive Bayes 2}
nb_cm$table |> knitr::kable()
```

KNN:
```{r KNN 2}
knn_cm$table |> knitr::kable()
```

Random forest:
```{r Random forest 2}
rf_cm$table |> knitr::kable()
```

CatBoost:
```{r CatBoost 2}
matrix(c(
  "", "canteen", "home", "other",
  "canteen", 0, 0, 0,
  "home", 0, 0, 0,
  "other", 0, 0, 0),
  nrow = 4,
  ncol = 4) |> knitr::kable()
```

sklearn is the transpose of caret







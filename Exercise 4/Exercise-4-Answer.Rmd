---
title: "ECO395M DM&SL Exercise 4"
author: "Sibo Ding"
date: "Spring 2024"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(dplyr)
library(ggplot2)
library(tidyr)

library(igraph)
library(arules)  # Association
library(arulesViz)
```

## Clustering and PCA
I want to use unsupervised learning to distinguish: 1. whether a wine is red or white; 2. the quality of a wine. I first standardize 11 variables (chemical properties).  
```{r standardize}
wine <- read.csv("wine.csv")
# Standardization
wine[, 1:11] <- scale(wine[, 1:11])
```
Remember in unsupervised learning, there is no features (x) and outcomes (y), no prediction, no right or wrong, no accuracy.

### Clustering
To distinguish red wines from white wines, I use K-means to cluster data into 2 clusters. Then I count the number of reds and whites in each cluster. In the below table, each row is a cluster, and each column is a color of wine. The algorithm can distinguish reds from whites, as most whites are in cluster 1 and most reds are in cluster 2.
```{r kmeans color}
set.seed(42)
kmeans_2 <- kmeans(wine[, 1:11], centers = 2)
table(kmeans_2$cluster, wine$color)
```

As there are 7 levels of quality (3 - 9), I use K-means to cluster data into 7 clusters. Then I count the quality in each cluster. In the below table, each row is a cluster, and each column is a quality level. The algorithm cannot distinguish the quality, as the quality spreads out among each cluster (row). There is no dominant quality for each cluster.
```{r kmeans quality 1}
set.seed(42)
kmeans_7 <- kmeans(wine[, 1:11], centers = 7)
table(kmeans_7$cluster, wine$quality)
```

Alternatively, I cluster data into 2 clusters to try to distinguish "high" vs. "low" quality. Similarly, the algorithm cannot distinguish the quality.
```{r kmeans quality 2}
table(kmeans_2$cluster, wine$quality)
```

### PCA
I project 11 variables onto 3 principle components (PC). It is acceptable to have more PCs, as the $(n+1)$-th PC will not affect all $n$ PCs. I calculate the projected values of all observations on each PC.  
```{r pca 1}
pca_res <- prcomp(wine[, 1:11], rank. = 3)
# Projected values of observations on each principle component
pp <- cbind(wine[, 12:13], pca_res$x)
```

I create a box plot of the projections on PC1 for reds and whites. The algorithm can distinguish reds from whites, as the projection for reds are mostly lower than those for whites.
```{r pca color plot}
ggplot(pp) +
  geom_boxplot(aes(color, PC1)) +
  ylab("Projection on PC1")
```

More rigorously, I conduct a t-test to compare the mean projection of reds and whites. Reds are significantly lower than whites.
```{r pca color t.test}
pc1_red <- pp |> filter(color == "red") |> select(PC1)
pc1_white <- pp |> filter(color == "white") |> select(PC1)
t.test(pc1_red, pc1_white)
```

I create a box plot of the projections on PC1 for different qualities. The algorithm cannot distinguish different qualities, as the projections are similar across them. Results are similar for PC2 and PC3.
```{r pca quality 1}
ggplot(pp) +
  geom_boxplot(aes(factor(quality), PC1)) +
  xlab("quality") +
  ylab("Projection on PC1")
```

Alternatively, I define the quality as "high" if it is greater than 5 and "low" otherwise. Then, I create a box plot of the projections on PC1 for high and low qualities. Similarly, the algorithm cannot distinguish the quality.
```{r pca quality 2}
pp <- pp |> mutate(qual_hl = ifelse(quality > 5, "high", "low"))

ggplot(pp) +
  geom_boxplot(aes(qual_hl, PC1)) +
  xlab("quality") +
  ylab("Projection on PC1")
```

## Market segmentation
I want to separate 7882 users into several market segments, based on interest categories of their Twitter posts. From the data summary below, most third quantiles are 1 or 2, and most maximums are above 10, meaning variables are highly right-skewed.
```{r data summary}
mkt <- read.csv("social_marketing.csv", row.names = 1)
summary(mkt)
```

Next, from the below correlation plot, there are 8 or 9 highly-correlated groups. However, it is not very informative and intuitive. For example, `food` is not highly correlated with `cooking`.  
```{r corr plot, fig.height=7, fig.width=7}
ggcorrplot::ggcorrplot(cor(mkt), hc.order = TRUE)
```

The information in 36 right-skewed categories is too sparse. Thus, I use PCA to summarize them into 3 PCs. There is no right or wrong for selecting the number of PCs, so you can try others if you want. I standardize 36 variables here.  
```{r pca 2}
pca_res1 <- prcomp(mkt, center = TRUE, scale. = TRUE, rank. = 3)
```

Then, I use K-means to cluster 3 projections of observations into 2 cluster. Again, there is no right or wrong for selecting the number of clusters. In practice, people often select the number of clusters based on their business problems.  
```{r kmeans}
# pca_res1$x: Projections of observations on each principle component
set.seed(42)
kmeans_res1 <- kmeans(pca_res1$x, centers = 2)

mkt <- mkt |>
  cbind(kmeans_res1$cluster) |>
  rename(cluster = "kmeans_res1$cluster")
```

I compare the average interests of all categories of two clusters. All interests for cluster 1 are higher than those for cluster 2, meaning users in cluster 1 are more active on Twitter. The company can focus on cluster 1 to have more active response.
```{r kmeans mean}
mkt |>
  group_by(cluster) |>
  summarize_all(mean) |>
  round(2) |>
  knitr::kable()
```

Below is the number of users in each cluster:
```{r count}
mkt |>
  group_by(cluster) |>
  count() |>
  rename(count = n) |>
  knitr::kable()
```

## Association rules for grocery purchases
I want to find association rules for customers' groceries shopping basket. That means to find associated groceries that customers are likely to buy, if they buy a certain grocery. I preprocess the data to fit into `arules` package.  
```{r arules preprocess}
groc <- read.csv("groceries.txt", header = FALSE)

# Add customer + Wide to long + Drop empty string
groc_df <- groc |>
  mutate(customer = row_number()) |>
  pivot_longer(cols = -customer,
               names_to = "name",
               values_to = "item") |>
  filter(item != "") |>
  select(-name)

# Factor
groc_df$customer <- factor(groc_df$customer)

# Split data into a list of items for each customer
groc_list <- split(x = groc_df$item, f = groc_df$customer)

# Convert to a special arules "transactions" class
groc_trans <- as(groc_list, "transactions")
```

When finding association rules, I choose the minimum *support* as 0.005 and the minimum *confidence* as 0.25. So *lift* = *confidence* / *support* = 50.  
- *Support* is the probability (proportion) of buying each grocery among all purchases. I choose a relatively low threshold to include more potential groceries.  
- *Confidence* is the conditional probability of buying a grocery, given buying another grocery. I choose a relatively high threshold to focus on major associations.  
- *Lift* measures the increased probability of buying a grocery, given buying another grocery.  
For example, a toothbrush is purchased 5% of the time in general, and 40% of the time when customers purchase a toothpaste. Then the *support* for toothbrush is 5%, the *confidence* is 40%, the *lift* is 8.  
```{r create association rules, results='hide'}
groc_rules <- apriori(groc_trans, 
	parameter = list(support = .005, confidence = .25, maxlen = 4))
```

Below shows the distribution of *support*, *confidence*, and *lift* of 23 rules.  
```{r plot rules dist}
plot(groc_rules)
```

Below shows the network. *whole milk* and *other vegetables* are at the center. Dairy products are associated with *whole milk*, and meats are associated with *other vegetables*.  
```{r plot network}
plot(groc_rules, method = "graph")
```

## Image classification with neural networks
See Jupyter Notebook
